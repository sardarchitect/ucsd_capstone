{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd4dbf6c",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c299959d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b626e04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/sardarchitect/repos/github.com/ucsd_capstone/')\n",
    "sys.path.append('/home/sardarchitect/repos/github.com/ucsd_capstone/streetstudy/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c95ffd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from streetstudy.data import virat\n",
    "from streetstudy.model import yolo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbc0ae3",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d26d899",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57647833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396a8633",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e30328f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>num_frames</th>\n",
       "      <th>duration</th>\n",
       "      <th>event_file</th>\n",
       "      <th>object_file</th>\n",
       "      <th>mapping_file</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>VIRAT_S_010200_08_000838_000867</th>\n",
       "      <td>/home/sardarchitect/repos/github.com/ucsd_caps...</td>\n",
       "      <td>532</td>\n",
       "      <td>22</td>\n",
       "      <td>VIRAT_S_010200_08_000838_000867.viratdata.even...</td>\n",
       "      <td>VIRAT_S_010200_08_000838_000867.viratdata.obje...</td>\n",
       "      <td>VIRAT_S_010200_08_000838_000867.viratdata.mapp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIRAT_S_000200_03_000657_000899</th>\n",
       "      <td>/home/sardarchitect/repos/github.com/ucsd_caps...</td>\n",
       "      <td>7243</td>\n",
       "      <td>241</td>\n",
       "      <td>VIRAT_S_000200_03_000657_000899.viratdata.even...</td>\n",
       "      <td>VIRAT_S_000200_03_000657_000899.viratdata.obje...</td>\n",
       "      <td>VIRAT_S_000200_03_000657_000899.viratdata.mapp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIRAT_S_050000_08_001235_001295</th>\n",
       "      <td>/home/sardarchitect/repos/github.com/ucsd_caps...</td>\n",
       "      <td>1792</td>\n",
       "      <td>59</td>\n",
       "      <td>VIRAT_S_050000_08_001235_001295.viratdata.even...</td>\n",
       "      <td>VIRAT_S_050000_08_001235_001295.viratdata.obje...</td>\n",
       "      <td>VIRAT_S_050000_08_001235_001295.viratdata.mapp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIRAT_S_050000_06_000908_000970</th>\n",
       "      <td>/home/sardarchitect/repos/github.com/ucsd_caps...</td>\n",
       "      <td>1855</td>\n",
       "      <td>61</td>\n",
       "      <td>VIRAT_S_050000_06_000908_000970.viratdata.even...</td>\n",
       "      <td>VIRAT_S_050000_06_000908_000970.viratdata.obje...</td>\n",
       "      <td>VIRAT_S_050000_06_000908_000970.viratdata.mapp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIRAT_S_000207_04_000902_000934</th>\n",
       "      <td>/home/sardarchitect/repos/github.com/ucsd_caps...</td>\n",
       "      <td>938</td>\n",
       "      <td>31</td>\n",
       "      <td>VIRAT_S_000207_04_000902_000934.viratdata.even...</td>\n",
       "      <td>VIRAT_S_000207_04_000902_000934.viratdata.obje...</td>\n",
       "      <td>VIRAT_S_000207_04_000902_000934.viratdata.mapp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                              path  \\\n",
       "name                                                                                 \n",
       "VIRAT_S_010200_08_000838_000867  /home/sardarchitect/repos/github.com/ucsd_caps...   \n",
       "VIRAT_S_000200_03_000657_000899  /home/sardarchitect/repos/github.com/ucsd_caps...   \n",
       "VIRAT_S_050000_08_001235_001295  /home/sardarchitect/repos/github.com/ucsd_caps...   \n",
       "VIRAT_S_050000_06_000908_000970  /home/sardarchitect/repos/github.com/ucsd_caps...   \n",
       "VIRAT_S_000207_04_000902_000934  /home/sardarchitect/repos/github.com/ucsd_caps...   \n",
       "\n",
       "                                 num_frames  duration  \\\n",
       "name                                                    \n",
       "VIRAT_S_010200_08_000838_000867         532        22   \n",
       "VIRAT_S_000200_03_000657_000899        7243       241   \n",
       "VIRAT_S_050000_08_001235_001295        1792        59   \n",
       "VIRAT_S_050000_06_000908_000970        1855        61   \n",
       "VIRAT_S_000207_04_000902_000934         938        31   \n",
       "\n",
       "                                                                        event_file  \\\n",
       "name                                                                                 \n",
       "VIRAT_S_010200_08_000838_000867  VIRAT_S_010200_08_000838_000867.viratdata.even...   \n",
       "VIRAT_S_000200_03_000657_000899  VIRAT_S_000200_03_000657_000899.viratdata.even...   \n",
       "VIRAT_S_050000_08_001235_001295  VIRAT_S_050000_08_001235_001295.viratdata.even...   \n",
       "VIRAT_S_050000_06_000908_000970  VIRAT_S_050000_06_000908_000970.viratdata.even...   \n",
       "VIRAT_S_000207_04_000902_000934  VIRAT_S_000207_04_000902_000934.viratdata.even...   \n",
       "\n",
       "                                                                       object_file  \\\n",
       "name                                                                                 \n",
       "VIRAT_S_010200_08_000838_000867  VIRAT_S_010200_08_000838_000867.viratdata.obje...   \n",
       "VIRAT_S_000200_03_000657_000899  VIRAT_S_000200_03_000657_000899.viratdata.obje...   \n",
       "VIRAT_S_050000_08_001235_001295  VIRAT_S_050000_08_001235_001295.viratdata.obje...   \n",
       "VIRAT_S_050000_06_000908_000970  VIRAT_S_050000_06_000908_000970.viratdata.obje...   \n",
       "VIRAT_S_000207_04_000902_000934  VIRAT_S_000207_04_000902_000934.viratdata.obje...   \n",
       "\n",
       "                                                                      mapping_file  \n",
       "name                                                                                \n",
       "VIRAT_S_010200_08_000838_000867  VIRAT_S_010200_08_000838_000867.viratdata.mapp...  \n",
       "VIRAT_S_000200_03_000657_000899  VIRAT_S_000200_03_000657_000899.viratdata.mapp...  \n",
       "VIRAT_S_050000_08_001235_001295  VIRAT_S_050000_08_001235_001295.viratdata.mapp...  \n",
       "VIRAT_S_050000_06_000908_000970  VIRAT_S_050000_06_000908_000970.viratdata.mapp...  \n",
       "VIRAT_S_000207_04_000902_000934  VIRAT_S_000207_04_000902_000934.viratdata.mapp...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_df = virat.build()\n",
    "video_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b0d268a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path            /home/sardarchitect/repos/github.com/ucsd_caps...\n",
       "num_frames                                                   9075\n",
       "duration                                                      302\n",
       "event_file                    VIRAT_S_000002.viratdata.events.txt\n",
       "object_file                  VIRAT_S_000002.viratdata.objects.txt\n",
       "mapping_file                 VIRAT_S_000002.viratdata.mapping.txt\n",
       "Name: VIRAT_S_000002, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_video = video_df.loc[\"VIRAT_S_000002\"]\n",
    "current_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92cc708b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>object_id</th>\n",
       "      <th>object_duration</th>\n",
       "      <th>current_frame</th>\n",
       "      <th>bbox_lefttop_x</th>\n",
       "      <th>bbox_lefttop_y</th>\n",
       "      <th>bbox_width</th>\n",
       "      <th>bbox_height</th>\n",
       "      <th>object_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>9076</td>\n",
       "      <td>0</td>\n",
       "      <td>1262</td>\n",
       "      <td>381</td>\n",
       "      <td>53</td>\n",
       "      <td>116</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>9076</td>\n",
       "      <td>1</td>\n",
       "      <td>1261</td>\n",
       "      <td>381</td>\n",
       "      <td>53</td>\n",
       "      <td>116</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>9076</td>\n",
       "      <td>2</td>\n",
       "      <td>1260</td>\n",
       "      <td>381</td>\n",
       "      <td>53</td>\n",
       "      <td>116</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>9076</td>\n",
       "      <td>3</td>\n",
       "      <td>1259</td>\n",
       "      <td>381</td>\n",
       "      <td>53</td>\n",
       "      <td>116</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>9076</td>\n",
       "      <td>4</td>\n",
       "      <td>1258</td>\n",
       "      <td>381</td>\n",
       "      <td>53</td>\n",
       "      <td>116</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   object_id  object_duration  current_frame  bbox_lefttop_x  bbox_lefttop_y  \\\n",
       "0          2             9076              0            1262             381   \n",
       "1          2             9076              1            1261             381   \n",
       "2          2             9076              2            1260             381   \n",
       "3          2             9076              3            1259             381   \n",
       "4          2             9076              4            1258             381   \n",
       "\n",
       "   bbox_width  bbox_height  object_type  \n",
       "0          53          116            1  \n",
       "1          53          116            1  \n",
       "2          53          116            1  \n",
       "3          53          116            1  \n",
       "4          53          116            1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations_df = virat.get_annotations(current_video['path'])\n",
    "annotations_df = annotations_df[annotations_df['object_type'] == 1]\n",
    "annotations_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327eaac2",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88d3995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee256f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/sardarchitect/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2023-4-30 Python-3.11.3 torch-2.0.0+cu117 CUDA:0 (NVIDIA GeForce GTX 1650, 4096MiB)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m /home/sardarchitect/.cache/torch/hub/requirements.txt not found, check failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "YOLOv5s summary: 166 layers, 7053910 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "clf = yolo.yolov5()\n",
    "clf.conf = 0\n",
    "clf.classes = [0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc53ffc",
   "metadata": {},
   "source": [
    "## Evaluation Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a192dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0.25\n",
      "0.25\n",
      "1.0\n",
      "tensor(0.25000)\n"
     ]
    }
   ],
   "source": [
    "def bbox_iou(boxA, boxB):\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    \n",
    "    intersection_width = xB - xA \n",
    "    intersection_height = yB - yA\n",
    "    \n",
    "    if intersection_width <= 0 or intersection_height <= 0:\n",
    "        return 0\n",
    "    \n",
    "    intersection_area = intersection_width * intersection_height\n",
    "    boxA_area = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxB_area = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "    \n",
    "    iou = intersection_area / float(boxA_area + boxB_area - intersection_area)\n",
    "    return iou   \n",
    "\n",
    "# TEST\n",
    "boxA = [0,0,10,10]\n",
    "boxB = [10,10,20,20]\n",
    "print(bbox_iou(boxA, boxB))\n",
    "\n",
    "boxA = [10,10,20,20]\n",
    "boxB = [0,0,10,10]\n",
    "print(bbox_iou(boxA, boxB))\n",
    "\n",
    "boxA = [0,0,10,10]\n",
    "boxB = [5,5,10,10]\n",
    "print(bbox_iou(boxA, boxB))\n",
    "\n",
    "boxA = [5,5,10,10]\n",
    "boxB = [0,0,10,10]\n",
    "print(bbox_iou(boxA, boxB))\n",
    "\n",
    "boxA = [0,0,10,10]\n",
    "boxB = [0,0,10,10]\n",
    "print(bbox_iou(boxA, boxB))\n",
    "\n",
    "boxA = torch.tensor([5,5,10,10])\n",
    "boxB = torch.tensor([0,0,10,10])\n",
    "print(bbox_iou(boxA, boxB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efa0a4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4494e371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001295997000852367"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tic = time.perf_counter()\n",
    "\n",
    "boxA = torch.tensor([5,5,10,10])\n",
    "boxB = torch.tensor([0,0,10,10])\n",
    "bbox_iou(boxA, boxB)\n",
    "\n",
    "toc = time.perf_counter()\n",
    "toc-tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b916d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize\n",
    "\n",
    "def match_bboxes(bbox_gt, bbox_pred, IOU_THRESH=0.01):\n",
    "    '''\n",
    "    Given sets of ground truth and predicted bounding boxes,\n",
    "    determine best possible match.\n",
    "    '''\n",
    "    num_gt = bbox_gt.shape[0]\n",
    "    num_pred = bbox_pred.shape[0]\n",
    "    MAX_DIST = 1.0\n",
    "    MIN_IOU = 0.0\n",
    "    \n",
    "    iou_matrix = np.zeros((num_gt, num_pred))\n",
    "    \n",
    "    for i in range(num_gt):\n",
    "        for j in range(num_pred):\n",
    "            iou_matrix[i, j] = bbox_iou(bbox_gt[i], bbox_pred[j])\n",
    "    \n",
    "    if num_pred > num_gt:\n",
    "        diff = num_pred - num_gt\n",
    "        iou_matrix = np.concatenate((iou_matrix, np.full((diff, num_pred), MIN_IOU)), axis=0)\n",
    "        \n",
    "    if num_gt > num_pred:\n",
    "        diff = num_gt - num_pred\n",
    "        iou_matrix = np.concatenate((iou_matrix, np.full((num_gt, diff), MIN_IOU)), axis=1)\n",
    "        \n",
    "    idxs_gt, idxs_pred = scipy.optimize.linear_sum_assignment(1 - iou_matrix)\n",
    "    if (not idxs_gt.size) or (not idxs_pred.size):\n",
    "        ious = np.array([])\n",
    "    else:\n",
    "        ious = iou_matrix[idxs_gt, idxs_pred]\n",
    "        \n",
    "    sel_pred = idxs_pred < num_pred\n",
    "    idx_pred_actual = idxs_pred[sel_pred]\n",
    "    idx_gt_actual = idxs_gt[sel_pred]\n",
    "    ious_actual = iou_matrix[idx_gt_actual, idx_pred_actual]\n",
    "    sel_valid = (ious_actual > IOU_THRESH)\n",
    "    label = sel_valid.astype(int)\n",
    "    \n",
    "    return idx_gt_actual[sel_valid], idx_pred_actual[sel_valid], ious_actual[sel_valid], label\n",
    "\n",
    "# TEST\n",
    "bbox_gt = np.array([[0,0,5,5], [10,10,25,25], [40,40,65,65]])\n",
    "bbox_pred = np.array([[0,0,4,5], [35,35,70,70], [0,0,1,1], [10,10,26,20]])\n",
    "ap = match_bboxes(bbox_gt, bbox_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a7d1610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64),\n",
       " array([], dtype=int64),\n",
       " array([], dtype=float64),\n",
       " array([0]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbox_gt = np.array([[0,0,5,5], [10,10,25,25], [40,40,65,65]])\n",
    "bbox_pred = np.array([[0,0,0,0]])\n",
    "match_bboxes(bbox_gt, bbox_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216aa1d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15479e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mAP = []\n",
    "# num_frames = int(current_video['num_frames'])\n",
    "# capture = cv.VideoCapture(current_video['path'])\n",
    "\n",
    "# for current_frame in tqdm(range(num_frames)):\n",
    "#     ret, frame = capture.read()\n",
    "#     preds = clf(frame)\n",
    "    \n",
    "#     bbox_pred = (preds.xyxy[0][:, :4]).cpu().numpy()\n",
    "#     bbox_gt = annotations_df[annotations_df['current_frame'] == current_frame].to_numpy()[:,3:7]\n",
    "#     bbox_gt[:, 2] = bbox_gt[:, 0] + bbox_gt[:, 2]\n",
    "#     bbox_gt[:, 3] = bbox_gt[:, 1] + bbox_gt[:, 3]\n",
    "\n",
    "#     mAP.append(match_bboxes(bbox_gt, bbox_pred)[3])\n",
    "# #     break\n",
    "    \n",
    "# capture.release()\n",
    "\n",
    "# TP = 0\n",
    "# TPFP = 0\n",
    "\n",
    "# for i in mAP:\n",
    "#     TPFP += len(i)\n",
    "#     TP += sum(i)\n",
    "# average_precision = TP/TPFP    \n",
    "# print(\"Average Precision:\", average_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1862b5c",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e3dc37",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cedc5e",
   "metadata": {},
   "source": [
    "## Testing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab46f039",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_df.sort_values('num_frames', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "287aa331",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc55ea7a65a142ce984764844d5e0912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34d782057eb4406bb08f310308b7e23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/362 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m current_frame \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(num_frames)):\n\u001b[1;32m     18\u001b[0m     _, frame \u001b[38;5;241m=\u001b[39m capture\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m---> 19\u001b[0m     preds \u001b[38;5;241m=\u001b[39m clf(frame)\n\u001b[1;32m     21\u001b[0m     bbox_pred \u001b[38;5;241m=\u001b[39m (preds\u001b[38;5;241m.\u001b[39mxyxy[\u001b[38;5;241m0\u001b[39m][:, :\u001b[38;5;241m4\u001b[39m])\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     22\u001b[0m     bbox_gt \u001b[38;5;241m=\u001b[39m (annotations_df[annotations_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcurrent_frame\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m current_frame]\u001b[38;5;241m.\u001b[39mto_numpy()[:,\u001b[38;5;241m3\u001b[39m:\u001b[38;5;241m7\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/mle/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/mle/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:708\u001b[0m, in \u001b[0;36mAutoShape.forward\u001b[0;34m(self, ims, size, augment, profile)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;66;03m# Post-process\u001b[39;00m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dt[\u001b[38;5;241m2\u001b[39m]:\n\u001b[0;32m--> 708\u001b[0m     y \u001b[38;5;241m=\u001b[39m non_max_suppression(y \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdmb \u001b[38;5;28;01melse\u001b[39;00m y[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    709\u001b[0m                             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconf,\n\u001b[1;32m    710\u001b[0m                             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miou,\n\u001b[1;32m    711\u001b[0m                             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses,\n\u001b[1;32m    712\u001b[0m                             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magnostic,\n\u001b[1;32m    713\u001b[0m                             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_label,\n\u001b[1;32m    714\u001b[0m                             max_det\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_det)  \u001b[38;5;66;03m# NMS\u001b[39;00m\n\u001b[1;32m    715\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[1;32m    716\u001b[0m         scale_boxes(shape1, y[i][:, :\u001b[38;5;241m4\u001b[39m], shape0[i])\n",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/utils/general.py:949\u001b[0m, in \u001b[0;36mnon_max_suppression\u001b[0;34m(prediction, conf_thres, iou_thres, classes, agnostic, multi_label, labels, max_det, nm)\u001b[0m\n\u001b[1;32m    947\u001b[0m mi \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m+\u001b[39m nc  \u001b[38;5;66;03m# mask start index\u001b[39;00m\n\u001b[1;32m    948\u001b[0m output \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m6\u001b[39m \u001b[38;5;241m+\u001b[39m nm), device\u001b[38;5;241m=\u001b[39mprediction\u001b[38;5;241m.\u001b[39mdevice)] \u001b[38;5;241m*\u001b[39m bs\n\u001b[0;32m--> 949\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m xi, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(prediction):  \u001b[38;5;66;03m# image index, image inference\u001b[39;00m\n\u001b[1;32m    950\u001b[0m     \u001b[38;5;66;03m# Apply constraints\u001b[39;00m\n\u001b[1;32m    951\u001b[0m     \u001b[38;5;66;03m# x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\u001b[39;00m\n\u001b[1;32m    952\u001b[0m     x \u001b[38;5;241m=\u001b[39m x[xc[xi]]  \u001b[38;5;66;03m# confidence\u001b[39;00m\n\u001b[1;32m    954\u001b[0m     \u001b[38;5;66;03m# Cat apriori labels if autolabelling\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mle/lib/python3.11/site-packages/torch/_tensor.py:931\u001b[0m, in \u001b[0;36mTensor.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration over a 0-d tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 931\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state():\n\u001b[1;32m    932\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterating over a tensor might cause the trace to be incorrect. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    934\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing a tensor of different shape won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt change the number of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    938\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    939\u001b[0m     )\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munbind(\u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "TOTAL_AP = []\n",
    "\n",
    "total_videos = 20\n",
    "current_video_number = 0\n",
    "\n",
    "for video_idx in tqdm(range(total_videos)):\n",
    "    \n",
    "    current_video = video_df.iloc[video_idx]    \n",
    "    annotations_df = virat.get_annotations(current_video['path'])\n",
    "    annotations_df = annotations_df[annotations_df['object_type'] == 1]\n",
    "        \n",
    "    num_frames = int(current_video['num_frames'])\n",
    "    AP = []\n",
    "    \n",
    "    capture = cv.VideoCapture(current_video['path'])\n",
    "    for current_frame in tqdm(range(num_frames)):\n",
    "\n",
    "        _, frame = capture.read()\n",
    "        preds = clf(frame)\n",
    "\n",
    "        bbox_pred = (preds.xyxy[0][:, :4]).cpu().numpy()\n",
    "        bbox_gt = (annotations_df[annotations_df['current_frame'] == current_frame].to_numpy()[:,3:7])\n",
    "        bbox_gt[:, 2] = bbox_gt[:, 0] + bbox_gt[:, 2]\n",
    "        bbox_gt[:, 3] = bbox_gt[:, 1] + bbox_gt[:, 3]\n",
    "        \n",
    "        AP.append(match_bboxes(bbox_gt, bbox_pred)[3])\n",
    "\n",
    "    capture.release()\n",
    "\n",
    "    TP = 0\n",
    "    TPFP = 0\n",
    "    for i in AP:\n",
    "        TPFP += len(i)\n",
    "        TP += sum(i)\n",
    "    \n",
    "    if TPFP == 0:\n",
    "        average_precision = 0\n",
    "    else:\n",
    "        average_precision = TP/TPFP\n",
    "    \n",
    "    TOTAL_AP.append(average_precision)\n",
    "    print(\"Average Precision:\", average_precision)\n",
    "    \n",
    "sum(TOTAL_AP) / total_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2cd810c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YOLOv5 <class 'models.common.Detections'> instance\n",
       "image 1/1: 720x1280 (no detections)\n",
       "Speed: 1.1ms pre-process, 13.2ms inference, 1.6ms NMS per image at shape (1, 3, 384, 640)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc4cc44",
   "metadata": {},
   "source": [
    "## Model Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732f0b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
